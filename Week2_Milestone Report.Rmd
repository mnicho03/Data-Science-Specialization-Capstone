---
title: "Exploratory Data Analysis of the SwiftKey Corpus"
author: "Michael Nichols"
date: "May 11, 2018"
output:
  html_document: default
  pdf_document: default
---
### *Data Science Specialization Capstone Project*

###Tasks to accomplish
1. Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.

2. Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r env_setup, message=FALSE, warning=FALSE, echo=FALSE}
#Environment Setup
#Load Data & R Packages

#set working directory
setwd("C:/Users/mnicho03/Desktop/Capstone_Local/final/en_US")

#load libraries
library(stringi) #for string manipulation
library(stringr) #for string manipulation
library(tm) #to filter out specific words
library(LaF) #to read specified percentage of the files
library(dplyr)
library(tidytext)
library(ggplot2)
library(gridExtra)
library(scales)
library(data.table) #fast file read/writes
library(kableExtra) #for pretty table outputs
library(wordcloud)
library(tm) #to filter out specific words
library(LaF) #to read specified percentage of the files

#for reproducibility
set.seed(16)
```

```{r large_data_load, message=FALSE, warning=FALSE, echo=FALSE}
# load in the full data files
en_files <- list.files()

#save separately
blogs <- readLines(en_files[1], encoding = "UTF-8", skipNul = TRUE)
news <- readLines(en_files[2], encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines(en_files[3], encoding = "UTF-8", skipNul = TRUE)

#early exploratory analysis

#identify overarching stats per .csv
blogsStats <- stri_stats_general(blogs)
newsStats <- stri_stats_general(news)
twitterStats <- stri_stats_general(twitter)

#more summary stats
#number of words by line
blogsWords <- stringi::stri_count_words(blogs)
newsWords <- stringi::stri_count_words(news)
twitterWords <- stringi::stri_count_words(twitter)

#basic description of the data
fileOverview <- data.frame(file = c("blogs", "news", "twitter"),
                           totalLines = c(stri_stats_general(blogs)[1], stri_stats_general(news)[1], stri_stats_general(twitter)[1]),
                           totalWords = c(sum(blogsWords), sum(newsWords), sum(twitterWords)),
                           totalChars = c(stri_stats_general(blogs)[3], stri_stats_general(news)[3], stri_stats_general(twitter)[3]),
                           averageWords = c(mean(blogsWords), mean(newsWords), mean(twitterWords)),
                           minWords = c(min(blogsWords), min(newsWords), min(twitterWords)),
                           maxWords = c(max(blogsWords), max(newsWords), max(twitterWords)),
                           averageChars = c(mean(stringi::stri_count_boundaries(blogs, type = "character")), mean(stringi::stri_count_boundaries(news, type = "character")), mean(stringi::stri_count_boundaries(twitter, type = "character"))),
                           minChars = c(min(stringi::stri_count_boundaries(blogs, type = "character")), min(stringi::stri_count_boundaries(news, type = "character")), min(stringi::stri_count_boundaries(twitter, type = "character"))),
                           maxChars = c(max(stringi::stri_count_boundaries(blogs, type = "character")), max(stringi::stri_count_boundaries(news, type = "character")), max(stringi::stri_count_boundaries(twitter, type = "character"))))

#tokenization: random sampling and file size reduction
#unneccessary to utilize entire files to analyze & build prediction models
#to offer significant enough size and consistency, we'll load in a random 5% of lines from each file

mini_blogs <- sample(blogs, length(blogs)*.05)
mini_news <- sample(news, length(news)*.05)
mini_twitter <- sample(twitter, length(twitter)*.05)

#profanity filtering
#load file of bad words
#list of profane words comes from Google's banned words list and comprises swear/curse/vulgar/innappropriate/offensive/dirty/rude/insulting words
temp <- tempfile()
download.file("https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-csv-file_2018_03_26_26.zip",temp)
profanity <- read.table(unz(temp, "full-list-of-bad-words-csv-file_2018_03_26.csv"))
unlink(temp)

#use removeWords from tm library to remove all profane words
# removeWords(str_to_lower(mini_en_US.blogs.txt), profanity)
clean_blogs <- removeWords(str_to_lower(mini_blogs), profanity[,1])
clean_news <- removeWords(str_to_lower(mini_news), profanity[,1])
clean_twitter <- removeWords(str_to_lower(mini_twitter), profanity[,1])

#initial exploratory analysis 
#step 1: ensure text is in DF form (will be utilizing clean, subsetted version of the original datasets for at least the dev portion of this phase)
# class(clean_blogs) # <- character class

clean_blogs_df <- data.frame(line = 1:length(clean_blogs),
                             text = clean_blogs, stringsAsFactors = FALSE)

clean_news_df <- data.frame(line = 1:length(clean_news),
                             text = clean_news, stringsAsFactors = FALSE)

clean_twitter_df <- data.frame(line = 1:length(clean_twitter),
                             text = clean_twitter, stringsAsFactors = FALSE)
# #save off mini files for speedy startup
# fwrite(clean_blogs_df, "clean_blogs_df.csv")
# fwrite(clean_news_df, "clean_news_df.csv")
# fwrite(clean_twitter_df, "clean_twitter_df.csv")
```

###Initial Exploratory Analysis

### *Descriptive Statistics: Blogs, News, Twitter*

*Based of the Full Datasets* 
```{r large_file_summary, echo=FALSE, fig.align="center", results = "asis", warning=FALSE}
#basic description of the data
fileOverview <- data.frame(file = c("blogs", "news", "twitter"),
                           totalLines = c(stri_stats_general(blogs)[1], stri_stats_general(news)[1], stri_stats_general(twitter)[1]),
                           totalWords = c(sum(blogsWords), sum(newsWords), sum(twitterWords)),
                           totalChars = c(stri_stats_general(blogs)[3], stri_stats_general(news)[3], stri_stats_general(twitter)[3]),
                           averageWords = c(round(mean(blogsWords), 2), round(mean(newsWords),2), round(mean(twitterWords),2)),
                           minWords = c(min(blogsWords), min(newsWords), min(twitterWords)),
                           maxWords = c(max(blogsWords), max(newsWords), max(twitterWords)),
                           averageChars = c(round(mean(stringi::stri_count_boundaries(blogs, type = "character")),2), round(mean(stringi::stri_count_boundaries(news, type = "character")),2), round(mean(stringi::stri_count_boundaries(twitter, type = "character")),2)),
                           minChars = c(min(stringi::stri_count_boundaries(blogs, type = "character")), min(stringi::stri_count_boundaries(news, type = "character")), min(stringi::stri_count_boundaries(twitter, type = "character"))),
                           maxChars = c(max(stringi::stri_count_boundaries(blogs, type = "character")), max(stringi::stri_count_boundaries(news, type = "character")), max(stringi::stri_count_boundaries(twitter, type = "character"))))

#print pretty table of the above
kable(fileOverview, format = "html") %>%
        kable_styling(bootstrap_options = "striped", font_size = 9) %>%
        column_spec(1, bold = TRUE) %>%
        column_spec(2:4, background = "lightgreen") %>%
        column_spec(5:7, background = "lightblue") %>%
        column_spec(8:10, background = "lightgoldenrodyellow") %>%
        add_header_above(c(" ", "Totals" = 3, "Words by Entry" = 3, "Characters by Entry" = 3))
```

<!-- ```{r mini_data_load} -->
<!-- #files created with 'GettingAndCleaning.R' file -->
<!-- #each DF is a mini, profanity-free version of the original:  -->
<!-- #randomly sampled 5% with all words from Google's banned words list removed -->
<!-- clean_blogs_df <- fread("clean_blogs_df.csv") -->
<!-- clean_news_df <- fread("clean_news_df.csv") -->
<!-- clean_twitter_df <- fread("clean_twitter_df.csv") -->
<!-- ``` -->

Create tokenized version of each dataset
```{r tokenized, message=FALSE, warning=FALSE}
#tokenize the data frames (split each word from each document into its own row)
#also remove 'stopwords': extremely common words not valuable for an analysis (e.g. as, of, the , a, ..., etc.)
tokenized_blogs <- clean_blogs_df %>%
        unnest_tokens(output = word, input = text) %>%
        anti_join(get_stopwords())

tokenized_news <- clean_news_df %>%
        unnest_tokens(output = word, input = text) %>%
        anti_join(get_stopwords())

tokenized_twitter <- clean_twitter_df %>%
        unnest_tokens(output = word, input = text) %>%
        anti_join(get_stopwords())
```

###Further Exploratory Visualizations

*Based of a randomly sampled 5% of the full datasets* 
```{r exploration, echo=FALSE, warning=FALSE, fig.align="center"}
# additional data prep
# simple count of words per group (excluding stopwords)
CommonWords_blogs <- tokenized_blogs %>%
        count(word, sort = TRUE) %>%
        mutate(file = "Blogs") %>%
        top_n(n = 10, wt = n)

CommonWords_news <- tokenized_news %>%
        count(word, sort = TRUE) %>%
        mutate(file = "News") %>%
        top_n(n = 10, wt = n)

CommonWords_twitter <- tokenized_twitter %>%
        count(word, sort = TRUE) %>%
        mutate(file = "Twitter") %>%
        top_n(n = 10, wt = n)

#create full DF to show info across files
CommonWords_all <- rbind.data.frame(CommonWords_blogs, CommonWords_news, CommonWords_twitter)

#plot top 10 most common words across the files
ggplot(CommonWords_all, aes(x = reorder(word,desc(n)), y = n, fill = n)) +
        geom_bar(stat = "identity", alpha = .95) +
        scale_fill_gradient(low = "gray75", high = "darkslategray4") +
        labs(y = "", x = "", title = "Top 10 Most Common Words by File") +
        facet_grid(.~file, scales = "free_x") +
        coord_flip() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
```

###N-gram Analysis
Per file, the following code gathers the top 10 most common 2-word (bigram) and 3-word (trigram) strings. This code ignore stop words (e.g. as, of, the , a, ..., etc.) to show drive more substantial insights.

*Based of a randomly sampled 5% of the full datasets* 
```{r n-gram, warning=FALSE}
#n-gram analysis
#bigrams
bigram_Analysis <- function(text, filetype) {
        text %>%
                unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
                tidyr::separate(bigram, c("word1", "word2"), sep = " ") %>%
                na.omit() %>%
                filter(!word1 %in% stop_words$word,
                       !word2 %in% stop_words$word) %>%
                count(word1, word2, sort = TRUE) %>%
                top_n(n = 10, wt = n) %>% #selects only top 10 values
                slice(row_number(1:10)) %>% #prevents ties (i.e. several bigrams with the 10th highest value)
                mutate(bigram = paste(word1, word2, sep = " ")) %>%
                mutate(file = filetype)
}

blogs_bigram <- bigram_Analysis(clean_blogs_df, "Blogs")
news_bigram <- bigram_Analysis(clean_news_df, "News")
twitter_bigram <- bigram_Analysis(clean_twitter_df, "Twitter")

#combine for easy visualizations
bigram_all <- as.data.frame(rbind.data.frame(blogs_bigram, news_bigram, twitter_bigram))

#trigrams
trigram_Analysis <- function(text, filetype) {
        text %>%
                unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
                tidyr::separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
                na.omit() %>%
                filter(!word1 %in% stop_words$word,
                       !word2 %in% stop_words$word,
                       !word3 %in% stop_words$word) %>%
                count(word1, word2, word3, sort = TRUE) %>%
                top_n(n = 10, wt = n) %>%
                slice(row_number(1:10)) %>%
                mutate(trigram = paste(word1, word2, word3, sep = " ")) %>%
                mutate(file = filetype)
}

blogs_trigram <- trigram_Analysis(clean_blogs_df, "Blogs")
news_trigram <- trigram_Analysis(clean_news_df, "News")
twitter_trigram <- trigram_Analysis(clean_twitter_df, "Twitter")

#combine for easy visualizations
trigram_all <- as.data.frame(rbind.data.frame(blogs_trigram, news_trigram, twitter_trigram))
```

### *Visualize the Bigrams & Trigrams*
```{r exploration2, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
ngram_plot <- function(data, num_gram) {
        label <- as.character(str_to_title(num_gram))
        
        ggplot(data, aes_string(x = num_gram)) +
                aes(y = n, fill = as.factor(file)) +
                geom_bar(stat = "identity") +
                facet_grid(file~., scales = "free_y") +
                coord_flip() +
                #labs(title = paste("Most Common", label, collapse = ""), y = label, x = "Occurences", fill = "File") +
                theme(axis.text.y = element_text(size = 8)) +
                theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), legend.position = "none")
        
}

top_bigrams <- ngram_plot(bigram_all, "bigram")
top_trigrams <- ngram_plot(trigram_all, "trigram")

grid.arrange(top_bigrams, top_trigrams, top = "Most Common Bigrams & Trigrams", ncol = 2)
```

### *Plot of concentration of certain words (only shows terms that make up .50 of the total)*

Plots with higher bars on the left have more unique terms (more instances where the frequency of a certain word is very small).

*Below the plot is a table displaying the number of unique words required to hit the specified percentages of all words in the text*

Hypothetical example to illustrate this concept: The word 'the' could occur 200 times in a 10,000 word document, thus consisting of 2% of all words in a document. 

*Based of a randomly sampled 5% of the full datasets* 
```{r exploration3, echo=FALSE, fig.align="center", results = "asis", message=FALSE, warning=FALSE}
#then look at term frequency (what # of terms consist of certain % of words)
runningTotal <- bind_rows(mutate(tokenized_blogs, file = "Blogs"),
                          mutate(tokenized_news, file = "News"), 
                          mutate(tokenized_twitter, file = "Twitter")) %>%
        count(file, word) %>%
        group_by(file) %>%
        mutate(proportion = n / sum(n)) %>%
        tidyr::spread(file, proportion) %>% 
        tidyr::gather(file, proportion, `Blogs`,`News`, `Twitter`) %>%
        na.omit() %>%
        arrange(desc(proportion)) %>%
        group_by(file) %>%
        mutate(cumsum = cumsum(proportion))

#plot of concentration of certain words (only shows terms that make up .50 of the total)
#plots with higher bars on the left have more unique terms
runningTotal_half <- runningTotal %>%
        filter(cumsum <= .5)

ggplot(runningTotal_half, aes(proportion, fill = as.factor(file))) +
        geom_histogram() +
        xlim(NA, 0.015) +
        facet_grid(.~file) +
        theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")

#table to show number of terms required to hit different percentages of total terms
#unique words for a frequency sorted dictionary to cover X% of all word instances
#e.g. 90% of terms from the Blogs text can be encompassed with only 16,035 terms 
percent_of_total_words <- runningTotal %>%
        select(file, cumsum) %>%
        group_by(file) %>%
        mutate(terms_to_percent = row_number()) %>% #number of terms required to hit the respective % of the total terms
        mutate(cumsum = round(cumsum, 2)) %>%
        filter(cumsum %in% c(0.10, 0.30, 0.50, 0.70, 0.90, 1.00)) %>%
        group_by(file, cumsum) %>%
        mutate(the_rank  = rank(-terms_to_percent, ties.method = "max")) %>%
        filter(the_rank == 1) %>% select(-the_rank) %>%
        arrange(file) %>%
        tidyr::spread(cumsum, terms_to_percent)

#data frame showing unique words required to hit each percentage of total words per text
percent_of_total_words <- as.data.frame(percent_of_total_words)

#print pretty table of the above
kable(percent_of_total_words, format = "html") %>%
        kable_styling("striped") %>%
        add_header_above(c(" ", "Unique Terms to Cover X Percent of Total Words" = 6))
```

##Questions to Consider
*1. How can you efficiently store an n-gram model (think Markov Chains)?*

Store the original corpus in n-gram tokenized data tables. Using the n-gram models, bits are dependent on the frequency of its occurrence after some other word(s). By using the Markov Assumption, we can simplify our process by assuming that future states in our model only depend upon the present state of our model. This assumption means that we can reduce our conditional probabilities to be approximately equal so that

P('rain'|'There was heavy') ~ P('rain'|'heavy')

More generally, we can estimate the probability of a sentence by the probabilities of each component part. In the equation that follows, the probability of the sentence is reduced to the probabilities of the sentence's individual bigrams.

P('There was heavy rain') ~ P('There')P('was'|'There')P('heavy'|'was')P('rain'|'heavy')

*2. How can you use the knowledge about word frequencies to make your model smaller and more efficient?*

Based on our analysis, we could utilize get 50% coverage of unique terms in the entire corpus by bringing the most popular between 5-15% of the unique terms. Simply put, we could completely ignore 'rare' words, and have a less comprehensive, highly efficient model.

*3. How many parameters do you need (i.e. how big is n in your n-gram model)?*

Tri-grams, at least for the initial model, will be the largest parameter. 

*4. Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data)?*

Additive (Laplace) smoothing: add 1 to all counts to avoid zero frequencies (Laplace Smoothing)

*5. How do you evaluate whether your model is any good?* 
Perplexity: measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.

*6. How can you use backoff models to estimate the probability of unobserved n-grams?*

Discounting: we can select an absolute discount which artificially lowers the counts of observed n-grams and distributes this "stolen" probabilty mass to unobserved n-grams.

###Appendix
### *Wordclouds*

Now looking at all the files together without first filtering for the most common words, create 3 wordcloud: individual word, bigrams, and trigrams. 
-- This visualization could be well suited for a final data product (Shiny app).

*Based of a randomly sampled 5% of the full datasets* 
```{r appendix, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
#create one aggregate tokenized file -- first need to include value for file type in each DF
#this DF includes the entire list of words from all files: this will be more useful than the above DF's used for initial plots
tokenized_all <- as.data.frame(rbind.data.frame(tokenized_blogs, tokenized_news, tokenized_twitter))

#create table of all the words
wordTable <- table(tokenized_all$word)

#wordcloud of all the words by frequency (reducing the total number of included words to a more palettable portion)
wordcloud(names(wordTable), as.numeric(wordTable), min.freq = 75, max.words = 500, colors = brewer.pal(8, "Dark2"))


#wordclouds for 2/3-gram analysis
clean_all <- as.data.frame(rbind.data.frame(clean_blogs_df, clean_news_df, clean_twitter_df))

#bigrams
#create DF of all bigrams and a count of each
bigram_all_unlimited <- 
        clean_all %>%
                unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
                tidyr::separate(bigram, c("word1", "word2"), sep = " ") %>%
                na.omit() %>%
                filter(!word1 %in% stop_words$word,
                       !word2 %in% stop_words$word) %>%
                count(word1, word2, sort = TRUE) %>%
                mutate(bigram = paste(word1, word2, sep = " "))

#bigram wordcloud
wordcloud(bigram_all_unlimited$bigram, bigram_all_unlimited$n, min.freq = 50, max.words = 250, colors = brewer.pal(8, "Dark2"), scale = c(2, .2))


#trigrams
#create DF of all trigrams and a count of each
trigram_all_unlimited <- 
        clean_all %>%
        unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
        tidyr::separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
        na.omit() %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word,
               !word3 %in% stop_words$word) %>%
        count(word1, word2, word3, sort = TRUE) %>%
        mutate(trigram = paste(word1, word2, word3, sep = " "))

#trigram wordcloud
wordcloud(trigram_all_unlimited$trigram, trigram_all_unlimited$n, min.freq = 1, max.words = 200, colors = brewer.pal(8, "Dark2"), scale = c(2, .2))
```
